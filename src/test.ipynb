{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CUDA is not available. Training on CPU\n"
     ]
    }
   ],
   "source": [
    "from util import create_dir, replace_dir, Clock, compare_dir, split_parameters\n",
    "from transform_img import flatten_onehot, Diff_size_collect, get_transform, norm_black_color\n",
    "from loss import Soft_dice_loss, Focal_loss, SSIM, activate\n",
    "from plot import plot_grad_flow, Progress_writer, onehot_gird, Loss_record, Acc_record, Loss_writer,LayerActivations\n",
    "from dataset.dataset import Image_Dataset, Zip_dataset, get_data_files\n",
    "from dataset.tarpath import Tar_path\n",
    "import accuracy as acc\n",
    "from torch.utils.data import DataLoader, Subset\n",
    "from torchsummary import summary\n",
    "\n",
    "from net.unet import U_Net\n",
    "from net.nested_unet import NestedUNet\n",
    "from net.regseg import RegSeg\n",
    "from net.regseg_p import RegSeg_dp\n",
    "\n",
    "import tarfile\n",
    "from pathlib import Path\n",
    "from itertools import islice\n",
    "import copy\n",
    "import time\n",
    "import cv2\n",
    "from matplotlib import pyplot as plt\n",
    "import torch.nn.functional as F\n",
    "import torchvision\n",
    "from torch.utils.tensorboard import SummaryWriter\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "from itertools import product\n",
    "\n",
    "from dataset.lmdb_format import Lmdb_dataset\n",
    "from dataset.supervisely_format import get_super_dataset\n",
    "from transform_img import get_transform, onehot_seq_torch, eq_proportion_resize, color_means, color_stds,transform_label\n",
    "\n",
    "# 检查cuda\n",
    "train_on_gpu = False # torch.cuda.is_available()\n",
    "\n",
    "if not train_on_gpu:\n",
    "    print('CUDA is not available. Training on CPU')\n",
    "    torch.set_num_threads(12)\n",
    "else:\n",
    "    print('CUDA is available. Training on GPU')\n",
    "\n",
    "device = torch.device(\"cuda:0\" if train_on_gpu else \"cpu\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMG_SIZE = 512\n",
    "\n",
    "# 数据转换\n",
    "tr_list = [\n",
    "    torchvision.transforms.Lambda(\n",
    "        lambda img:eq_proportion_resize(img, float(IMG_SIZE), cv2.INTER_CUBIC)),\n",
    "    torchvision.transforms.Lambda(\n",
    "        lambda img:cv2.cvtColor(img, cv2.COLOR_BGR2RGB)),\n",
    "    torchvision.transforms.ToTensor(),\n",
    "    torchvision.transforms.Normalize(\n",
    "        mean=color_means,\n",
    "        std=color_stds, inplace=True)\n",
    "]\n",
    "transform = torchvision.transforms.Compose(tr_list)\n",
    "\n",
    "\n",
    "def get_collect_fn(channels):\n",
    "    gd_black = torch.zeros((channels, 1, 1))\n",
    "    if channels > 1:\n",
    "        gd_black[0, ...] = torch.tensor([1])\n",
    "\n",
    "    def collect_fn(imgs):\n",
    "        return Diff_size_collect.collect_fn(\n",
    "            imgs, 4,\n",
    "            black={\n",
    "                0: torch.tensor(norm_black_color).reshape((3, 1, 1)),\n",
    "                1: gd_black\n",
    "            })\n",
    "    return collect_fn\n",
    "\n",
    "\n",
    "def get_data_loader_para(channels):\n",
    "    return {\n",
    "        \"batch_size\": 1,\n",
    "        \"shuffle\": False,\n",
    "        \"pin_memory\": train_on_gpu,\n",
    "        \"num_workers\": 1,\n",
    "        \"collate_fn\": get_collect_fn(channels),\n",
    "    }\n",
    "\n",
    "\n",
    "def test_dataset(dataset):\n",
    "    for x, y in islice(dataset, 4):\n",
    "        plt.figure()\n",
    "        plt.subplot(211)\n",
    "        plt.imshow(x.permute((1, 2, 0)))\n",
    "        plt.subplot(212)\n",
    "        plt.imshow(onehot_gird(y).permute((1, 2, 0)))\n",
    "        plt.show()\n",
    "\n",
    "\n",
    "# 加载数据集\n",
    "# Persons Labeled\n",
    "persons_labeled = get_super_dataset(\n",
    "    \"../data/graduate/Supervisely Person Dataset\", transform,\n",
    "    torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Lambda(\n",
    "            lambda img:eq_proportion_resize(img, float(IMG_SIZE), cv2.INTER_NEAREST)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Lambda(lambda img:img*255),\n",
    "    ]))\n",
    "persons_labeled_loader = DataLoader(\n",
    "    persons_labeled, **get_data_loader_para(1)\n",
    ")\n",
    "\n",
    "# ATR\n",
    "unuse_label = []\n",
    "\n",
    "\n",
    "def delete_label(img: np.ndarray, labels: list[int]):\n",
    "    for l in labels:\n",
    "        img[img == l] = 0\n",
    "    img[img > 0] = 1\n",
    "    return img\n",
    "\n",
    "\n",
    "atr_img = Image_Dataset(get_data_files(\n",
    "    \"../data/graduate/LIP/ATR/humanparsing/JPEGImages\"), transform)\n",
    "atr_tr = Image_Dataset(get_data_files(\n",
    "    \"../data/graduate/LIP/ATR/humanparsing/SegmentationClassAug\"),\n",
    "    torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Lambda(\n",
    "            lambda img:eq_proportion_resize(img, float(IMG_SIZE), cv2.INTER_NEAREST)),\n",
    "        torchvision.transforms.Lambda(\n",
    "            lambda img:delete_label(img, unuse_label)),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Lambda(lambda img:img*255),\n",
    "    ]),\n",
    "    cv2.IMREAD_GRAYSCALE)\n",
    "atr_dataset = Zip_dataset(atr_img, atr_tr)\n",
    "atr_loader = DataLoader(atr_dataset, **get_data_loader_para(1))\n",
    "\n",
    "#\n",
    "val_img_dataset = Lmdb_dataset(\n",
    "    \"../data/graduate/lip_c5_db\", \"validation\",\n",
    "    torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        torchvision.transforms.Normalize(\n",
    "            mean=color_means,\n",
    "            std=color_stds, inplace=True)\n",
    "    ])\n",
    ")\n",
    "val_target_img_dataset = Lmdb_dataset(\n",
    "    \"../data/graduate/lip_c5_db\", \"validation_seg\",\n",
    "    torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Lambda(\n",
    "            lambda img:torch.tensor(np.expand_dims(img, 0), dtype=torch.float32)),\n",
    "        torchvision.transforms.Lambda(\n",
    "            lambda img: onehot_seq_torch(img, 6, torch.float32)),\n",
    "    ])\n",
    ")\n",
    "\n",
    "val_dataset = Zip_dataset(val_img_dataset, val_target_img_dataset)\n",
    "val_loader = DataLoader(val_dataset, **get_data_loader_para(6))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# 加载模型\n",
    "from net.gate_t0 import RegSeg_gate0\n",
    "\n",
    "\n",
    "class RegSeg_gate0_6(RegSeg_gate0):\n",
    "    C12_TO_C5 = {\n",
    "        1: [1, 2],\n",
    "        2: [4, 12],\n",
    "        3: [5, 6, 9, 10],\n",
    "        4: [7, 8, 11],\n",
    "        5: [3,],\n",
    "    }\n",
    "\n",
    "    def forward(self, x: torch.Tensor):\n",
    "        o = super().forward(x)\n",
    "        return transform_label(o, self.C12_TO_C5)\n",
    "\n",
    "\n",
    "regseg_gate0 = RegSeg_gate0_6(3, 13)\n",
    "regseg_gate0.load_state_dict(torch.load(\n",
    "    \"../model/complete/RegSeg_gatet0_3to13_e50_b8_s512/model/model_e45.pth\", map_location=device))\n",
    "regseg_gate0.to(device)\n",
    "\n",
    "regseg_align = RegSeg_dp(3, 6, 0)\n",
    "regseg_align.load_state_dict(torch.load(\n",
    "    \"../model/complete/RegSeg_align_3to6_e50_b8_s512/model/model_e29.pth\", map_location=device))\n",
    "regseg_align.to(device)\n",
    "\n",
    "regseg = RegSeg(2, 6)\n",
    "regseg.load_state_dict(torch.load(\n",
    "    \"../model/complete/RegSeg_3to6_e200_b8_s512/model/model_e196.pth\", map_location=device))\n",
    "regseg.to(device)\n",
    "\n",
    "regseg_gate = RegSeg_dp(3, 6, 1)\n",
    "regseg_gate.load_state_dict(torch.load(\n",
    "    \"../model/complete/RegSeg_gate_3to6_e50_b8_s512/model/model_e40.pth\", map_location=device))\n",
    "regseg_gate.to(device)\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "test gate0\n",
      "data Person labeled\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomoyo/.local/lib/python3.10/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "times: 0.1218\n",
      "class iou:\n",
      "\t92.61%\n",
      "\t77.16%\n",
      "mean iou: 84.88%\n",
      "front iou: 77.16%\n",
      "mean pa: 91.02%\n",
      "pa: 94.98%\n"
     ]
    }
   ],
   "source": [
    "def test(model: torch.nn.Module, dataloader: DataLoader, calculate_acc):\n",
    "\n",
    "    use_time = 0.\n",
    "\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        for batch_index, (x, y) in enumerate(dataloader):\n",
    "            x, y = x.to(device), y.to(device)\n",
    "\n",
    "            t = time.time()\n",
    "            prediction: torch.Tensor = model(x)\n",
    "            use_time += time.time() - t\n",
    "\n",
    "            prediction = activate(prediction)\n",
    "\n",
    "            if prediction.size(1) != y.size(1):\n",
    "                if prediction.size(1) > 1 and y.size(1) == 1:\n",
    "                    prediction = torch.argmax(\n",
    "                        prediction, dim=1, keepdim=True) > 0\n",
    "                    prediction = prediction.to(torch.int32)\n",
    "                    y = y.to(torch.int32)\n",
    "                elif prediction.size(1) == 1:\n",
    "                    prediction = prediction > 0.5\n",
    "                    prediction = prediction.to(torch.int32)\n",
    "\n",
    "                    if y.size(1) != 1:\n",
    "                        y = torch.argmax(y, dim=1, keepdim=True) > 0\n",
    "                        y = y.to(torch.int32)\n",
    "            else:\n",
    "                prediction = torch.argmax(prediction, dim=1, keepdim=True)\n",
    "                y = torch.argmax(y, dim=1, keepdim=True)\n",
    "\n",
    "            calculate_acc(prediction, y, batch_index)\n",
    "\n",
    "    return use_time\n",
    "\n",
    "\n",
    "def test_model(model: torch.nn.Module, data: DataLoader, class_):\n",
    "    acc_r = Acc_record(len(data.dataset), max(class_, 2))\n",
    "    time = test(model, data,\n",
    "                lambda p, y, b: acc_r.calculate(p, y))\n",
    "\n",
    "    print(\"times: {:.4f}\".format(time / len(data.dataset)))\n",
    "\n",
    "    print(\"class iou:\"+\n",
    "          \"\".join((\"\\n\\t{:.2%}\".format(c.mean()) for c in acc_r.class_iou))\n",
    "        )\n",
    "\n",
    "    print(\"mean iou: {:.2%}\".format(acc_r.mean_iou.mean()))\n",
    "    print(\"front iou: {:.2%}\".format(acc_r.front_iou.mean()))\n",
    "\n",
    "    print(\"mean pa: {:.2%}\".format(acc_r.cpa.mean(0).mean()))\n",
    "    print(\"pa: {:.2%}\".format(acc_r.pa.mean()))\n",
    "    return acc_r\n",
    "\n",
    "\n",
    "for (model_name, model), (name, d, n_class) in product(\n",
    "    (\n",
    "        # (\"regseg\", regseg),\n",
    "        # (\"align\", regseg_align),\n",
    "        # (\"gate\", regseg_gate),\n",
    "        (\"gate0\", regseg_gate0),\n",
    "    ),\n",
    "    (\n",
    "        (\"Person labeled\", persons_labeled_loader, 1),\n",
    "        # (\"ATR\", atr_loader, 1),\n",
    "        # (\"VAL\", val_loader, 6),\n",
    "    )\n",
    "):\n",
    "    print(\"test {}\\ndata {}\".format(model_name, name))\n",
    "    test_model(model, d, n_class)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Successfully created '../tmp/regseg_test_pl' \n",
      "Successfully created '../tmp/regseg_test_pl/compare' \n",
      "Successfully created '../tmp/regseg_test_pl/gate' \n",
      "Successfully created '../tmp/regseg_test_pl/flow' \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/tomoyo/.local/lib/python3.10/site-packages/torch/nn/functional.py:4227: UserWarning: Default grid_sample and affine_grid behavior has changed to align_corners=False since 1.3.0. Please specify align_corners=True if the old behavior is desired. See the documentation of grid_sample for details.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "STDS_ = torch.tensor(color_stds).view(1, 3, 1, 1).to(device)\n",
    "MEANS_ = torch.tensor(color_means).view(1, 3, 1, 1).to(device)\n",
    "\n",
    "\n",
    "def unnormalize(img: torch.Tensor):\n",
    "    return img * STDS_.to(img.device) + MEANS_.to(img.device)\n",
    "\n",
    "\n",
    "def zoom(img: torch.Tensor, t_min=0., t_max=1.):\n",
    "    max_ = img.max()\n",
    "    min_ = img.min()\n",
    "    return img / (max_-min_) * (t_max-t_min) + t_min\n",
    "\n",
    "\n",
    "def predict(model: torch.nn.Module, x: torch.Tensor):\n",
    "    model.eval()\n",
    "    with torch.no_grad():\n",
    "        x = x.to(device)\n",
    "\n",
    "        prediction: torch.Tensor = model(x)\n",
    "        prediction = activate(prediction)\n",
    "\n",
    "        return prediction\n",
    "\n",
    "\n",
    "def show(prediction: torch.Tensor, x: torch.Tensor):\n",
    "\n",
    "    if prediction.size(1) > 1:\n",
    "        prediction = torch.argmax(prediction, dim=1, keepdim=True)\n",
    "    else:\n",
    "        prediction = prediction > 0.5\n",
    "        prediction = prediction.to(torch.int32)\n",
    "\n",
    "    xp = unnormalize(x)\n",
    "    idx = (prediction > 0).expand_as(xp)\n",
    "\n",
    "    xp[~idx] *= 0.1\n",
    "    idx = idx.clone()\n",
    "    idx &= torch.tensor([1, 0, 0], dtype=torch.bool,\n",
    "                        device=device).view(1, 3, 1, 1)\n",
    "    xp[~idx] += 0.2\n",
    "\n",
    "    prediction = prediction.to(torch.float32)\n",
    "\n",
    "    w = torch.tensor([[-1, 0], [0, 1]],\n",
    "                     dtype=torch.float32).view(1, 1, 2, 2).to(device)\n",
    "    edge = F.conv2d(prediction, w, padding=\"same\")\n",
    "    idx = (edge > 0).expand_as(xp)\n",
    "    xp[idx] = 1\n",
    "\n",
    "    w = torch.tensor([[0, -1], [1, 0]],\n",
    "                     dtype=torch.float32).view(1, 1, 2, 2).to(device)\n",
    "    edge = F.conv2d(prediction, w, padding=\"same\")\n",
    "    idx = (edge > 0).expand_as(xp)\n",
    "    xp[idx] = 1\n",
    "\n",
    "    return xp\n",
    "\n",
    "\n",
    "align16_layer = LayerActivations(regseg_align.decoder.up16.flow_make)\n",
    "align8_layer = LayerActivations(regseg_align.decoder.up8.flow_make)\n",
    "\n",
    "\n",
    "def show_flow():\n",
    "    flow16 = torchvision.utils.flow_to_image(align16_layer.features)\n",
    "    flow8 = torchvision.utils.flow_to_image(align8_layer.features)\n",
    "    return flow16, flow8\n",
    "\n",
    "\n",
    "gate16_layer = LayerActivations(regseg_gate0.decoder.fusion_8_16.h_conv)\n",
    "gate8_layer = LayerActivations(regseg_gate0.decoder.fusion_4_8.h_conv)\n",
    "\n",
    "\n",
    "def show_gate():\n",
    "    gate16 = gate16_layer.features\n",
    "    gate16 = torch.sigmoid_(gate16)\n",
    "    gate8 = gate8_layer.features\n",
    "    gate8 = torch.sigmoid_(gate8)\n",
    "\n",
    "    gate16 = gate16.mean(1, keepdim=True)\n",
    "    gate8 = gate8.mean(1, keepdim=True)\n",
    "    return gate16, gate8\n",
    "\n",
    "\n",
    "SAVE_PATH = Path(\"../tmp\")\n",
    "\n",
    "p = SAVE_PATH/\"regseg_test_pl\"\n",
    "compare_img_save_path = p/\"compare\"\n",
    "gate_img_save_path = p/\"gate\"\n",
    "flow_img_save_path = p/\"flow\"\n",
    "\n",
    "for path in p, compare_img_save_path, gate_img_save_path, flow_img_save_path:\n",
    "    create_dir(path)\n",
    "\n",
    "for i, (x, y) in enumerate(persons_labeled_loader):\n",
    "    x = x.to(device)\n",
    "    seg_p = predict(regseg, x)\n",
    "    seg_effect = show(seg_p, x)\n",
    "\n",
    "    align_p = predict(regseg_align, x)\n",
    "    align_effect = show(align_p, x)\n",
    "\n",
    "    gate_p = predict(regseg_gate0, x)\n",
    "    gate_effect = show(gate_p, x)\n",
    "\n",
    "    gate16, gate8 = show_gate()\n",
    "    gate16 = F.interpolate(gate16, x.size()[-2:]).expand_as(x)\n",
    "    gate8 = F.interpolate(gate8, x.size()[-2:]).expand_as(x)\n",
    "\n",
    "    flow16, flow8 = show_flow()\n",
    "    flow16 = F.interpolate(flow16, x.size()[-2:])\n",
    "    flow8 = F.interpolate(flow8, x.size()[-2:])\n",
    "\n",
    "    x = unnormalize(x)\n",
    "\n",
    "    for imgs, path in (\n",
    "        ([x, seg_effect, align_effect, gate_effect], compare_img_save_path),\n",
    "        ([x, gate8, gate16], gate_img_save_path),\n",
    "        ([x, flow8, flow16], flow_img_save_path),\n",
    "    ):\n",
    "        torchvision.utils.save_image(\n",
    "            [img.squeeze(0) for img in imgs],\n",
    "            path/\"{}.jpg\".format(i),\n",
    "            pad_value=0.5, normalize=True, scale_each=True)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "31f2aee4e71d21fbe5cf8b01ff0e069b9275f58929596ceb00d14d90e3e16cd6"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
